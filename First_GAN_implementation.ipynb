{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC GAN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.misc import imshow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change this if needed\n",
    "use_cuda = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 20, 24, 24])\n",
      "torch.Size([1, 20, 12, 12])\n",
      "torch.Size([1, 50, 8, 8])\n",
      "torch.Size([1, 50, 4, 4])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 500])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        if verbose: print(\"Discriminator\")\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        if verbose: print(x.shape)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        if verbose: print(x.shape)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "# Let's test your decoder\n",
    "n_components = 2\n",
    "discriminator = Discriminator()\n",
    "y = discriminator(torch.randn(1, 1, 28, 28), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 3136])\n",
      "torch.Size([1, 16, 14, 14])\n",
      "torch.Size([1, 6, 14, 14])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    # YOUR CODE HERE\n",
    "    def __init__(self, n_components=2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_components, 250)\n",
    "        self.fc2 = nn.Linear(250, 14*14*16)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=16, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=6, out_channels=1, kernel_size=5, padding=2, stride=2, output_padding=1)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        if verbose: print('Decoder')\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = x.view((-1, 16, 14, 14))\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        if verbose: print(x.shape)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "# Let's test your Generator\n",
    "n_components = 2\n",
    "decoder_test = Generator(n_components)\n",
    "y = decoder_test(torch.randn(1, n_components), verbose=True)\n",
    "assert y.shape == torch.Size([1, 1, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_real = np.arange(900).reshape((30, 30))\n",
    "\n",
    "\n",
    "\n",
    "# imshow(y.detach().numpy().reshape((28, 28)))\n",
    "\n",
    "# ax.imshow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_neg = 100+image_real\n",
    "fig.set_data(image_neg)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-16:\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/u/53/aftenim1/unix/.conda/envs/capsenv/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-7acf659d6b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mnist_cnn.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-7acf659d6b7b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# TODO visualize generated images by the Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-7acf659d6b7b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(D, G, train_loader, epoch, optimizer_d, optimizer_g, debug)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mcombined_d_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss_fake\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_loss_real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mcombined_d_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/capsenv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "targets_real = torch.ones(64, 1).to(device)  # Targets for discriminator: real data\n",
    "targets_fake = torch.zeros(64, 1).to(device)  # Targets for discriminator: fake data\n",
    "criterion = nn.BCELoss()\n",
    "latent_size = 2\n",
    "\n",
    "def train(D, G, train_loader, epoch, optimizer_d, optimizer_g, debug=False):\n",
    "    # Initialize train mode\n",
    "    D.train()\n",
    "    G.train()\n",
    "    # train on batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device) #, target.to(device)\n",
    "#         data shape:  torch.Size([64, 1, 28, 28])\n",
    "        print(batch_idx)\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "        n_samples = len(data)\n",
    "        if debug: print('data shape: ', data.shape)\n",
    "        # update the discriminator\n",
    "        for i in range(10):\n",
    "            z_d = torch.randn(n_samples, latent_size).to(device)\n",
    "            data_fake = G(z_d)\n",
    "            # fake outputs\n",
    "            outputs_fake = D(data_fake)\n",
    "            # real outputs\n",
    "            outputs_real = D(data)\n",
    "            if debug: print('outputs fake:', outputs_fake.shape)\n",
    "            if debug: print('outputs real_data', outputs_real.shape)\n",
    "            outputs_real = D(data)\n",
    "            d_loss_fake = criterion(outputs_fake, targets_fake)\n",
    "            d_loss_real = criterion(outputs_real, targets_real)\n",
    "            combined_d_loss = d_loss_fake + d_loss_real\n",
    "            combined_d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "        \n",
    "        # Train the generator\n",
    "        # Compute loss with fake data\n",
    "        z = torch.randn(n_samples, latent_size).to(device)\n",
    "        data_fake = G(z)\n",
    "        outputs = D(data_fake)\n",
    "        g_loss = criterion(outputs, targets_real)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "def main():\n",
    "    seed = 1\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=64, shuffle=True, **kwargs)\n",
    "    \n",
    "    D = Discriminator().to(device)\n",
    "    G = Generator(2).to(device)\n",
    "    optimizer_d = optim.Adam(D.parameters(), lr=0.0002)\n",
    "    optimizer_g = optim.Adam(G.parameters(), lr=0.0002)\n",
    "    for epoch in range(1, 1000 + 1):\n",
    "        print(1)\n",
    "        train(D, G, train_loader, epoch, optimizer_d, optimizer_g)\n",
    "        # TODO visualize generated images by the Generator\n",
    "        if epoch % 50 == 0:\n",
    "            n_components = 2\n",
    "            decoder_test = Generator(n_components)\n",
    "            y = decoder_test(torch.randn(1, n_components), verbose=True)\n",
    "            fig = plt.imshow(y.detach().numpy().reshape((28, 28)), cmap='gray')\n",
    "            plt.show()\n",
    "        print('epoch', epoch, 'finished')\n",
    "    if (True):\n",
    "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capsenv",
   "language": "python",
   "name": "capsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
